(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{260:function(t,a,s){"use strict";s.r(a);var n=s(0),r=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"数据爬虫的设计与实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据爬虫的设计与实现"}},[t._v("#")]),t._v(" 数据爬虫的设计与实现")]),t._v(" "),s("p",[t._v("数据爬虫需要完成两个功能, 一是直接获取今日头条的数据, 二是在今日头条数据的基础上模拟用户阅读新闻.")]),t._v(" "),s("h2",{attrs:{id:"技术栈"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#技术栈"}},[t._v("#")]),t._v(" 技术栈")]),t._v(" "),s("ul",[s("li",[t._v("Python")]),t._v(" "),s("li",[t._v("Selenium 自动化测试工具")]),t._v(" "),s("li",[t._v("Requests HTTP 工具包")])]),t._v(" "),s("h2",{attrs:{id:"数据爬取"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据爬取"}},[t._v("#")]),t._v(" 数据爬取")]),t._v(" "),s("p",[t._v("依照实际需要, 系统将爬取 18 种不同类别的新闻, 每种类别下有 10 篇左右的新闻. 除此之外, 系统还将爬取新闻的作者信息, 每篇新闻下的评论, 评论的作者, 以及每则评论下的回复, 回复的作者.")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://github.com/iMeiji/Toutiao/wiki/%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1Api%E5%88%86%E6%9E%90#%E8%8E%B7%E5%8F%96%E6%96%B0%E9%97%BB%E8%AF%84%E8%AE%BA",target:"_blank",rel:"noopener noreferrer"}},[t._v("一个 GitHub 上对今日头条接口分析的资料"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("而为了降低系统的复杂度, 每篇新闻最多只会爬取 20 则评论信息, 每则评论下最多会爬取 10 则回复.")]),t._v(" "),s("p",[t._v("在个人水平的限制下, 我所知的爬取数据的手段有两种:")]),t._v(" "),s("ul",[s("li",[t._v("一是直接在渲染好的页面的 DOM 树下获取, 这个速度比较慢, 可以使用 "),s("a",{attrs:{href:"https://selenium.dev/documentation/zh-cn/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Selenium"),s("OutboundLink")],1),t._v(" 这类工具来完成")]),t._v(" "),s("li",[t._v("另一种是组装请求头, 假装浏览器发送请求获取数据.")])]),t._v(" "),s("p",[t._v("那么, 我们的这个爬虫需要的数据包括 文章, 文章的作者, 评论, 评论的作者, 回复, 回复的作者. 至于为什么不把其他的用户拉进来, 因为今日头条的用户数量过于庞大, 而且相互关联, 形成网状, 全部拉下来是不可能的. 而且相比于数据的数量, 我认为更重要的是数据的完整性和纯洁性. 这些数据被爬取下来后将被存储在一个 MySql 数据库中.")]),t._v(" "),s("p",[t._v("考虑到时间成本, 只对文章内容采用在 DOM 树下爬取的方式 (文章内容没法使用请求, 其实是可以的, 但是得益于头条的反扒机制, 这种方式是不稳定的), 其他都使用伪装请求的方式.")]),t._v(" "),s("h2",{attrs:{id:"模拟用户行为"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模拟用户行为"}},[t._v("#")]),t._v(" 模拟用户行为")]),t._v(" "),s("p",[t._v("首先说明一下, 个人认为, 模拟用户行为不是一种多么理想的操作, 它可能会带来很多的副作用. 至于为什么要模拟用户行为, 是因为数据爬虫只能获取哪些用户评论了新闻, 哪些用户回复了新闻, 至于这篇新闻到底有多少人浏览过, 这就无从查起了. 而前面提到过, 每篇新闻最多只会有 20 则评论信息, 每则评论下最多只有 10 则回复. 如此一条新闻下的用户行为数据不会超过 2 * 200, 绝大部分的用户只会在我们的数据库中出现两次用户行为记录(一次阅读, 一次评论; 或一次阅读, 一次回复; 或一次阅读, 一次编辑), 这对推荐系统来说, 体验是挺差的.")]),t._v(" "),s("h3",{attrs:{id:"模拟方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模拟方式"}},[t._v("#")]),t._v(" 模拟方式")]),t._v(" "),s("p",[t._v('核心就是 "随机", 另一个需要注意的是, 模拟的只有用户的阅读行为. 获取的评论或回复的用户, 作为幸运儿, 会被系统选中, 被阅读. 系统会先随机选择几个(一般是 3 个以内, 太多, 数据库可能吃不消) 新闻类别, 随后系统会使用一个 for 循环, 在每个类别新闻下选取随机数量的新闻 (一般是 40 篇以内).')]),t._v(" "),s("div",{staticClass:"language-Python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 随机选取 [1, 2] 数量的类别")]),t._v("\nrand_category_num "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randint"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nrand_cates "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("categories"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rand_category_num"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" rand_cate "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" rand_cates"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取同类别下随机数量的新闻 id 列表")]),t._v("\n    result_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__art_dao"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_same_category_art"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("art_mod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("art_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rand_cate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" result_list "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" back_art "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" result_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更新相关的用户 新闻统计数据")]),t._v("\n                self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__cus_dao"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("insert_cus_behavior"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("com_cus_mod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cus_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" back_art"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" back_art"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" back_art"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__cus_dao"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_cus_feature"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rand_cate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" com_cus_mod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cus_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" update_num"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__art_dao"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update_art_feature"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" back_art"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" art_mod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("art_time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("continue")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br")])]),s("p",[t._v("利用这样的操作, 可以只用几次数据爬取便能获取大量的活跃用户.")]),t._v(" "),s("h3",{attrs:{id:"副作用的考虑"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#副作用的考虑"}},[t._v("#")]),t._v(" 副作用的考虑")]),t._v(" "),s("p",[t._v("考虑没有模拟数据的情况, 系统评价一篇新闻的质量通常有三个指标: 发布时间, 评论数量, 回复数量, 因为系统在计算新闻分数的时候, 给这三个参数比较大的权重, 所以一篇文章的质量基本由这三个参数说了算, 而且这也基本符合正常的情况: 评论回复多的文章, 浏览数量通常不低.")]),t._v(" "),s("p",[t._v("但是现在, 每篇文章可能会有上千次的用户浏览记录, 原先的三个参数对新闻分数的影响一下子成了忽略不计的东西, 这就破坏了新闻的真实特征. 一种可能的解决办法就是降低阅读权重, 让这四个参数平起平坐, 但是我并没有用这个操作.")]),t._v(" "),s("p",[t._v("另外一个有意思的问题是, 如果只是随机一个类别下的新闻, 一开始被记录到数据库中的新闻往往会得到更多的用户阅读数据, 一种不完美的解决办法是对随机阅读的新闻的时间做出限制, 比如只允许随机阅读距当前 10 天的新闻. 非常不幸的一点是, 系统使用的接口获取的新闻的时间跨度广到离谱, 在模拟数据的过程中会失去一部分新闻.")]),t._v(" "),s("h2",{attrs:{id:"爬虫程序流程的简单说明"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#爬虫程序流程的简单说明"}},[t._v("#")]),t._v(" 爬虫程序流程的简单说明")]),t._v(" "),s("p",[t._v("在开始的时候, 我计划使用 process 和 dao 这两层, 就像 Spring 中的 service 和 dao 一样. 在我以这种思路写了一遍之后, 我发现这样的操作就是脱裤子放屁. 于是, process 不再包裹 dao, 单纯地负责页面内容和请求内容的获取, 并填写 model 对象; dao 则负责所有与数据库操作相关的内容, 接收填写好的 model 对象.")]),t._v(" "),s("p",[t._v("Python 就是 Python, 不要总想着写成别的语言的样子, 对, 我说的就是你 Java .")]),t._v(" "),s("h3",{attrs:{id:"程序数据处理顺序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#程序数据处理顺序"}},[t._v("#")]),t._v(" 程序数据处理顺序")]),t._v(" "),s("ul",[s("li",[t._v("新闻作者信息")]),t._v(" "),s("li",[t._v("新闻内容")]),t._v(" "),s("li",[t._v("评论作者")]),t._v(" "),s("li",[t._v("评论内容")]),t._v(" "),s("li",[t._v("评论作者模拟操作")]),t._v(" "),s("li",[t._v("回复作者")]),t._v(" "),s("li",[t._v("回复内容")]),t._v(" "),s("li",[t._v("回复作者模拟操作")])])])}),[],!1,null,null,null);a.default=r.exports}}]);