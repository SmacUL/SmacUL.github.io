# 推荐算法 异常 日志 用户行为

这是本章的几个主题. 

## 基于协同过滤算法的推荐系统

在毕业设计开题答辩的催促下, 我从知网和 GitHub 搞下了几份论文和项目, 大概地看了一下. 也就基本确定了推荐系统主要使用协同过滤算法, 而且主要是通过相似用户推荐. 那么不可避免地, 我就要计算两个用户之间的相似度. 问题来了: 如何计算? 

我参考了一下大家的做法, 计算相似度需要有用户向量, 而用户向量, 以我现在的想法, 我希望是用户对每一类新闻的偏好 (可以用一个数字表示) 组成的固定维度的向量. 接下来的事情可能会用到归一化 平均平方差之类的东西, 都好说. 

在这个思想的指导下, 我认为需要一个用户行为日志表来专门记录用户的行为, 包括写作 点赞 点踩 评论和回复. 每次需要计算用户偏好, 或者说用户特征模型, 的时候就会依赖这张日志表来拼接出某个用户对新闻的所有行为, 然后疯狂计算. 

::: tip
这里的主要推荐算法, 即计算用户之间的相似度, 把相似用户看过的新闻推荐过来. 
:::

当然一个合格的推荐系统还需要考虑冷启动的问题, 我对这个的理解不是很清晰, 暂时理解: 一个没有任何用户登录系统之后, 系统如何做推荐? 不管是不是冷启动, 这都要做, 我现在的想法是推荐那些大家看了都说好的新闻和热点新闻. 

剩下一个问题, 我还没有什么想法: 如何评价推荐算法? 这个我真的没有答案. 

## 项目结构的调整

### process 与 dao 的重新分工
在开始的时候, 我们提到过 process 和 dao 这两层, 就像 Spring 中的 service 和 dao 一样. 在我以这种思路写了一遍之后, 我发现这样的操作就是脱裤子放屁. 于是, process 不再包裹 dao, 单纯地负责页面内容和请求内容的获取, 并填写 model 对象; dao 则负责所有与数据库操作相关的内容, 接收填写好的 model 对象. 

``` Python
# 新闻用户处理
art_cus_mod = CusMod.CustomerModel()
try:
    self.__cus_pro.set_art_cus(art_brief_json, art_cus_mod)
    self.__cus_dao.group_check_insert_cus_then_search_id(art_cus_mod)
except:
    print("art_cus 处理 失败")
    continue
# 新闻处理
art_mod = ArtMod.ArticleModel()
try:
    self.__art_pro.set_art(art_brief_json, category, art_cus_mod.cus_id, art_mod)
    if not self.__art_dao.is_art_exist(art_mod.art_spider):
        # 新闻不存在的情况
        self.__art_dao.insert_art(art_mod)
    else:
        print("art 已存在")
        continue
    art_mod.art_id = self.__art_dao.search_art_id_by_spider(art_mod.art_spider)
except:
    print("art 操作 失败")
    continue
# 用户行为
try:
    if self.__art_dao.check_art_cus_relationship(art_mod.art_id, art_cus_mod.cus_id):
        self.__cus_dao.insert_cus_behavior(1, art_mod.art_id, art_cus_mod.cus_id, art_mod.art_time)
    else:
        pass
except:
    print("art-cus 行为 1 数据库操作 失败")
    continue
```

### 分类爬取
我发现了按照种类获取新闻的 API , 于是乎, 相关内容的获取就可以以种类为单位, 一组一组地扒. 就现在看到的数据来说, 新闻种类要更加丰富均匀, 文章的重复率也低了很多. 

``` Python
categories = ['news_society', 'news_entertainment', 'news_tech', 
                'news_military', 'news_sports', 'news_car',
                'news_finance', 'news_world', 'news_fashion', 
                'news_travel', 'news_discovery', 'news_baby',
                'news_regimen', 'news_story', 'news_essay', 
                'news_game', 'news_history', 'news_food']
for category in categories:
    print("\n当前类别: %s" % category)
    ... ...
```

### 精简配置
另外, 我还删掉了一堆没什么卵用的配置流程, 直接写死在代码. 
``` Python
if __name__ == '__main__':
    # 现在配置完数据库就可以直接上路了. 
    Major(os.path.join('properties', 'database.json')).major()
```
``` Python
# 这是以前 Major 类的配置
def __init__(self):
    self.__base: MySql.MySql = None

    self.__art_dao: ArtDao.ArticleDao = None
    self.__cus_dao: CusDao.CustomerDao = None
    self.__com_dao: ComDao.CommentDao = None
    self.__rep_dao: RepDao.ReplyDao = None

    self.__art_pro: ArtPro.ArticleProcess = None
    self.__cus_pro: CusPro.CustomerProcess = None
    self.__com_pro: ComPro.CommentProcess = None
    self.__rep_pro: RepPro.ReplyProcess = None

    self.__start: int = None
    self.__end: int = None
    self.__total: int = None

def init_database(self, path):
    db_map = Json.Json.read_json_file(path)
    self.__base = MySql.MySql(db_map['db_name'], db_map['user'], db_map['password'], print_sql=db_map['print_sql'])

def init_dao(self):
    self.__art_dao = ArtDao.ArticleDao(self.__base)
    self.__cus_dao = CusDao.CustomerDao(self.__base)
    self.__com_dao = ComDao.CommentDao(self.__base)
    self.__rep_dao = RepDao.ReplyDao(self.__base)

def init_process(self):
    self.__art_pro = ArtPro.ArticleProcess(self.__art_dao)
    self.__cus_pro = CusPro.CustomerProcess(self.__cus_dao)
    self.__com_pro = ComPro.CommentProcess(self.__com_dao)
    self.__rep_pro = RepPro.ReplyProcess(self.__rep_dao)

def set_process_scope(self, path):
    scope_map = Json.Json.read_json_file(path)
    self.__start = scope_map['start']
    self.__end = scope_map['end']
    self.__total = scope_map['total']

```
``` Python
# 这是现在 Major 类的配置
def __init__(self, path):
    db = Json.Json.read_json_file(path)
    self.__base = MySql.MySql(db_name=db['name'], user=db['user'], password=db['pass'],
                                host=db['host'], charset=db['charset'])

    self.__art_pro = ArtPro.ArticleProcess()
    self.__rep_pro = RepPro.ReplyProcess()
    self.__com_pro = ComPro.CommentProcess()
    self.__cus_pro = CusPro.CustomerProcess()

    self.__cus_dao = CusDao.CustomerDao(self.__base)
    self.__art_dao = ArtDao.ArticleDao(self.__base)
    self.__com_dao = ComDao.CommentDao(self.__base)
    self.__rep_dao = RepDao.ReplyDao(self.__base)
```

## 异常处理

主要是 try except 的使用. 不过具体的异常类, 我看的不多. 在当前这个项目中, 每一个 model 的 process 部分和 dao 部分会用 try except 包裹起来, 同时每个方法的内部也都添加了 try except , 在必要的时候, 方法的 Exception 会被 raise 给上一级. 

实际上 try except 的使用是一件有些矛盾的事情, 一方面我们希望提高 try except 的覆盖率和定位的准确度, 但是另一方面我们又要努力减少 try except 带来的额外的代码量. 

## print 与 logging

这个需求, 主要是之前不管是什么类型的信息, 都用 `print` 打印在 console 中, 流程简单还好, 像我们这样的基本是灾难. 所以使用 Python 自带的 `logging` 模块添加日志, 同时输出到日志文件中. 从我简单的使用经验看来, `logging` 和 `time` 两个模块可以是好朋友. 

虽然仍然使用 `print` , 但是它的主要是提示一下程序运行到了什么位置, 程序中有没有错误, 出错的程度是什么样的. 具体的错误信息就转到了日志文件中. 每次运行都会生成一个独立的日志文件. 

``` Python
# logging 的配置
import os.path
import logging
logging.basicConfig(level=logging.INFO, filemode='a', filename=os.path.join('log', '%s.txt' % Time.Time.get_local_time()))
```
日志结果示例: 
```
INFO:root:execute_sql select count(*) from Replys where rep_spider = '6761341170342772750'
INFO:root:get_result_one 获得数据: 0
INFO:root:is_rep_exist 回复 rep_spider=6761341170342772750 数据库查询 不存在
WARNING:root:search_rep_rep_by_spyder rep_reply_id 与 rep_type 数据库查询 失败
INFO:root:execute_sql insert into Replys(rep_content, rep_like_num, rep_type, rep_time, rep_customer_id, rep_article_id, rep_comment_id,  rep_spider, rep_legal) values ('加油哦，要坚持哈~', 0, 0, '2019-11-20 18:56:58', 2090, 144, 991, '6761341170342772750', 1)
INFO:root:commit_transactions
```

不过, 还是有问题. 当前的日志不能将内容分开来, 输出到不同的文件中; 单次的日志数据量有些大; 日志要是有颜色和链接就更好了. 

## 用户行为

至于用户行为, 直接看 SQL 吧:
``` SQL
DROP TABLE IF EXISTS NewsRecommend.ArticleCustomerBehaviors;
CREATE TABLE NewsRecommend.ArticleCustomerBehaviors (
    acb_id INT UNSIGNED NOT NULL auto_increment,
    -- acb_behavior 用户行为: 无 0, 写作 1, 点赞 2, 点踩 3, 评论 4, 回复 5
    acb_behavior INT UNSIGNED default 0,
    acb_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    acb_article_id INT UNSIGNED,
    acb_customer_id INT UNSIGNED,
	primary key(acb_id),
    foreign key(acb_article_id) references Articles(art_id),
    foreign key(acb_customer_id) references Customers(cus_id)
);
```
为了降低复杂度, 我只考虑到用户与文章之间的关系, 要是有(没)机(有)会(的), 我会细到用户与评论间的关系, 也应该会考虑新闻与评论间的关系. 

## 总结

通过这次的修改, 我觉得可以说收获这些东西:  
- GitHub 开源项目与论文对未知项目的作用
- 学习了 Python 的一批新语法
- 不要总想着让一门语言活成另一门语言的样子, 不能生搬硬套

当然问题也有, 现在的这种模式, `major` 方法的长度偏长, 不好处理. 

另外一个比较大问题, 我没办法保证数据一定是干净的. 理论上在插入用户行为之前会做一下检查: 这个行为是否真的存在在数据库中, 如果不在也就没有插入的必要, 同时还要将那些 "多余" 的数据删掉. 但是如果删除的过程中出现了意外, 程序终止或异常, 删除不成功岂不很尴尬? 我现在没有什么比较有效的手段防范, 只能立下一个约定: **所有的数据是否真实需要 (存在正确的数据关系) 以用户行为表中的内容为准**. 
