# 新闻数据爬虫

## 谈谈历史

这个玩意一开始是新闻推荐系统的一部分, 用来爬取今日头条的文章信息. 从它出生到现在, 已经经历过很多次的改版, 需求也发生了比较大的变化. 一开始我认为这个爬虫只要获取新闻数据及文章作者的相关数据就可以了, 但是后期仔细地看一了下数据, 既然是做推荐, 怎么能少了新闻的受众, 也就是普通用户. 于是我打算重写一遍这个爬虫. 

## 操作流程

### 总览

在个人水平的限制下, 我所知的爬取数据的手段有两种: 
- 一是直接在渲染好的页面的 DOM 树下获取, 这个速度比较慢, 可以使用 [Selenium](https://selenium.dev/documentation/zh-cn/) 工具来完成
- 另一种是组装请求头, 假装浏览器发送请求获取数据. 

那么, 我们的这个爬虫需要的数据包括 文章, 文章的作者, 评论, 评论的作者, 回复, 回复的作者. 至于为什么不把其他的用户拉进来, 因为今日头条的用户数量过于庞大, 而且相互关联, 形成网状, 全部拉下来是不可能的. 而且相比于数据的数量, 我认为更重要的是数据的完整性和纯洁性. 这些数据被爬取下来后将被存储在一个 MySql 数据库中. 

考虑到时间成本, 只对文章内容采用在 DOM 树下爬取的方式 (文章内容没法使用请求), 其他都使用伪装请求的方式. 

我们可以从首页先获取今日头条文章的缩略信息, 每组缩略信息中应该包含 8~10 条新闻的链接. 像这样: 

![](/note/img/2019-12-19-16-20-59.png)

通过链接, 我们可以获得文章的内容, 这不难, 比较麻烦的是评论和回复的内容, 尤其是回复, 回复包括对评论的回复和对回复的回复. 底下的图就是评论回复的样式. 

![](/note/img/2019-12-19-16-23-57.png)

### 大致流程

那么现在我们假设获取了这些链接, 并且以一篇文章为一个单位, 简单的想一想流程: 
1. 获取文章作者
2. 获取文章内容
1. 获取评论作者
1. 获取评论内容
1. 获取回复作者
1. 获取回复内容
1. 查看回复是对回复的回复还是对评论的回复

前面的步骤内容如果获取失败, 那么后面的内容也就没有处理的必要了. 同时评论和回复相关数据的获取, 在文章粒度下, 是需要循环处理的. 

总的来说, 这是一个数据处理的任务, 就暂时交给 python 处理了. 

## 详细点的分析
我们要的是一个服务于推荐系统的爬虫, 那么需要从大致的推荐手段开始考虑, 而推荐的主要内容是文章, 主要受众是普通用户. 

::: tip
用户比较特殊, 包括能够写作的用户和普通用户.  
:::

### 简单的推荐机制

#### 普通用户的行为
1. 给文章点赞/点踩
2. 给文章写评论
3. 给评论写回复
4. 给回复写回复
5. 给评论点赞/点踩
6. 给回复点赞/点踩
1. 关注他人

普通用户的行为会直接影响文章 评论 回复 文章作者的受欢迎程度. 在数据库中, 文章 评论 回复三个表都添加了 score 字段, 他们的作用是判别对应内容的受欢迎程度. 

#### 个人偏好
个人偏好, 一部分是由用户自己设定的, 另一部分是由系统对用户行为分析之后决定的. 未来还会有一个专门的表用来记录用户对某一类特定文章的偏好程度, 不过这个表和现在的爬虫没有关系, 不应该先入为主地对获得的数据进行处理, 应该保证数据的纯洁性. 

#### 推荐小结
文章的受欢迎程度, 一些固定的推荐机制和用户个人的偏好设置将最终决定推荐的内容. 实际的推荐系统还会考虑长尾效应的影响, 这里不考虑. 

### 具体的数据结构
下面的这些结构分别对应一张数据表, 具体的底下有. 为了方便爬虫获取与判断, 实际的数据表中会添加一些特殊的 id 字段. 另外考虑到后面的管理系统对网页内容的审核, 还需要添加与合法性相关的字段. 

#### 用户
除了基本的用户数据外, 要能够区分普通用户和写作用户, 也就是用户级别, 还包括用户关注的人数和用户的粉丝数量等. 用户表也是其他的表的基石. 

#### 文章
包括文章的标题, 内容, 描述内容, 点赞/点踩数量, 评论数量 (要依据实际获取到的评论数量), 作者的 id 信息, 文章类别和关键字等. 

#### 评论
评论的数据复杂度要小于文章, 可以看做是阉割版的文章数据. 今日头条中给出了每条评论的点赞数量, 这个数量应该被直接获取. 

#### 回复
回复表应该是整个数据库中外键最多的表, 它会和自己, 评论, 文章, 用户都创建关系. 和评论一样, 回复的点赞数量也会被直接获取. 

## 系统

- [源码](https://github.com/SmacUL/NewsRecommend/tree/master/crawler)
- [系统相关的数据库配置与表信息](https://github.com/SmacUL/NewsRecommend/blob/master/NewsRecommend.sql)

### 系统相关的数据库表

就和上面说的一样, 我们需要考虑四个要素: 用户, 文章, 评论, 回复. 底下给出了四个表以及相关的注释信息

#### 用户
``` sql
CREATE TABLE NewsRecommend.Customers (
    cus_id INT UNSIGNED NOT NULL auto_increment,
    cus_name VARCHAR(64),
    cus_pass VARCHAR(255),
    -- 用户的 url , 爬虫中用于识别用户
    cus_url VARCHAR(255) default '',
    -- 用户头像的 url
    cus_avatar_url VARCHAR(255) default '',
    -- 用户背景墙的图片 url
    cus_background_url VARCHAR(255) default '',
    -- 用户的个人描述
    cus_style VARCHAR(255) default '这个人很懒, 什么都没写',
    -- cus_gender 为 0 时性别未知, 为 1 时为男, 为 -1 时为女
    cus_gender TINYINT DEFAULT 0,
    -- 用户的创建时间
    cus_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    -- cus_type 为 0 时是普通用户, 为 1 时是可编辑用户
    cus_type TINYINT default 0,
    -- 此用户的关注的用户数量
    cus_follow_num int UNSIGNED default 0,
    -- 此用户的粉丝
    cus_fan_num int UNSIGNED default 0, 
    -- 此用户的文章数量
    cus_article_num int UNSIGNED default 0,
    -- 用户评分
    cus_scope int UNSIGNED default 0,
    -- cus_legal 为 0 时待审核, 为 1 时合法, 为 -1 不合法
    cus_legal TINYINT default 0,
	
    primary key(cus_id)
);
```

#### 文章(新闻)
``` SQL
CREATE TABLE NewsRecommend.Articles (
    art_id INT UNSIGNED NOT NULL auto_increment,
    art_title VARCHAR(255) default '',
    art_content TEXT,
    -- 文章的 url , 在爬虫中分辨文章
    art_url VARCHAR(255) default '',
    -- 文章的分类
    art_class VARCHAR(16) default '综合',
    -- 文章的标签 应该以 & 分隔
    art_tags VARCHAR(128) default '',
    -- 文章缩略图的信息
    art_image_url VARCHAR(255) default '',
    -- 文章的点赞数量
    art_like_num INT UNSIGNED default 0,
    -- 文章的点踩数量
    art_dislike_num INT UNSIGNED default 0,
    art_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    -- 文章的评论数量
    art_comment_num INT UNSIGNED default 0,
    -- 文章的分数
	art_scope int UNSIGNED default 0,
    art_legal tinyint default 0,
    
    art_customer_id INT UNSIGNED,
    primary key(art_id),
	foreign key(art_customer_id) references Customers(cus_id)
);
```

#### 评论
``` SQL
CREATE TABLE NewsRecommend.Comments (
    com_id INT UNSIGNED NOT NULL auto_increment,
    com_content TEXT,
    com_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    com_like_num INT UNSIGNED default 0,
    com_dislike_num INT UNSIGNED default 0,
	-- 评论的回复数量
    com_reply_num INT UNSIGNED default 0,
    -- 评论的分数
	com_scope int UNSIGNED default 0,
    com_legal tinyint default 0,
    -- 爬虫过程中的评论标识
    com_identify_id varchar(64) default '',
    
    com_customer_id INT UNSIGNED,
    com_article_id INT UNSIGNED, 
	primary key(com_id),
    foreign key(com_customer_id) references Customers(cus_id),
	foreign key(com_article_id) references Articles(art_id)
);
```

#### 回复
``` SQL
CREATE TABLE NewsRecommend.Replys (
    rep_id INT UNSIGNED NOT NULL auto_increment,
    rep_content TEXT,
    -- 回复的类型, 0 是对评论的回复, 1 是对回复的回复
    rep_type tinyint default 0, 
    rep_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    rep_like_num INT UNSIGNED default 0,
    rep_dislike_num INT UNSIGNED default 0,
	-- 回复的回复数量
    rep_reply_num INT UNSIGNED default 0,
    -- 回复的分数
	rep_scope int UNSIGNED default 0,
	rep_legal tinyint default 0,

    -- 爬虫过程中的评论标识
    rep_identify_id varchar(64) default '',

    rep_customer_id INT UNSIGNED,
    rep_article_id INT UNSIGNED,
    rep_comment_id INT UNSIGNED, 
    rep_reply_id INT UNSIGNED,
    primary key(rep_id),
    foreign key(rep_customer_id) references Customers(cus_id),
	foreign key(rep_article_id) references Articles(art_id),
    foreign key(rep_comment_id) references Comments(com_id),
    foreign key(rep_reply_id) references Replys(rep_id)
);
```



### 系统结构

- process   
    对应常见的 service , 处理一个小的流程
- dao   
    处理与数据库相关的操作
- model   
    类似 bean , 存放基本的数据类
- util  
    一些公共的工具包
- properties  
    存放基本的配置文件, 包括数据库, 数据获取范围, 请求信息等. 
- Main.py   
    万恶之源


### 工作流程

从 Main.py 开始, 不像 Spring / SpringBoot 有现成的对象管理容器, 这个系统的对象依赖还是只能手工添加. 所以, 在正式工作前, 我们需要先检查 process dao 数据库连接对象之间的依赖关系, 并且确定运行范围. 

``` Python
major = Major()
major.init_database(os.path.join('properties', 'database.json'))
major.init_dao()
major.init_process()
major.set_process_scope(os.path.join('properties', 'scope.json'))
major.major_process()
```

::: tip
其实这些步骤是没有分开写的必要的, 可以直接写到 Major 类中的 `__init__` 方法内, 这里只是个人喜好. 
:::

`major_process` 方法中就是一个完整的包含了总多 process 的流程. 数据先从 Main 到 process 再到 dao 最后前往数据库, 这之间可能需要 model 进行完整的数据对象的转运, 也可能需要 util 包中的工具支持. 

``` Python
for data in news_data:
    # 获取文章作者
    art_customer_url = self.__cus_pro.get_article_customer_url(data)
    if not self.__cus_pro.is_customer_exist(art_customer_url):
        self.__cus_pro.insert_art_customer(data, art_customer_url)
    art_customer_id = self.__cus_pro.get_customer_id_by_url(art_customer_url)
    if art_customer_id is None:
        continue
    print("文章作者处理完成\n")
```
这是 `major_process` 方法中的一段, 用于获取文章作者的相关内容, 这就像是作文里面的自然段. 其他内容的获取基本上也是这个结构. 

- 3 行用来获取文章作者个人中心的 url , 这是在爬虫阶段区分用户的字段; 
- 4 行会判断这个用户是否存在于数据库中, 如果不存在, 则执行 5 行内容;
- 5 行用来获取这个用户的相关信息, 并且插入数据到数据库中; 
- 6 行会获取用户的 id , 这是在实际系统中用来区分用户的字段; 
- 7~8 行, 如果 6 行中的 id 没有获取, 可能是没有被正确插入, 总之是出现了异常, 那么后面一堆的数据都不需要获取了, 直接下一篇; 

## 总结

今日头条的数据爬取难度比我想象中要大的多, 这个系统看起来, 尤其是 `major_process` 方法中定义的流程还是非常杂乱的, 也比较脆弱, 对异常和日志的管理就是灾难. 也许有一天, 在我深入了 Python 的语法, 同时了解更多的设计模式之后, 可以写出看起更好的代码. 