# 第一章

## 人工智能简史

- 起源  
    - 1950，Alan Turing在论文《计算机器与智能》中提出了图灵测试，在过去的数十年间被认为是测试机器智能的重要标准
        > 图灵测试：一名人类测试员分别通过文字与两个密室中的一台机器和一个人自由对话，如果测试员无法判断哪个密室中的是机器，那么这台机器就被认为通过了图灵测试。
    - 1951，Marvin Minsky建立了世界上第一个神经网络机器SNARC
    - 1955，逻辑理论家的计算机程序出现，开创了一种日后被广泛应用的方法：搜索推理
    - 1956，达特茅斯会议举行，正式宣告人工智能作为一门学科的诞生
        > 这次会议提出：学习和智能的每一个方面都能够被精确地描述，使得人们可以制造一台机器来模拟它。

- 第一次浪潮  
    20世纪60年代  
    第一次浪潮在70年代退去主要有几个原因：
    - 人们对人工智能的期望过高，逐渐失去耐心
    - 计算能力有限
    - 当时的技术难以解决视觉和自然语言理解巨大的可变性和模糊性等问题

- 第二次浪潮
    20世纪80年代，两类技术有了新的突破：
    - 专家系统  
        用基于一组特定规则来回答问题特定领域问题的程序系统。
        > 专家系统也改变了人工智能的发展方向，人们开始专注于通过智能系统来解决具体领域的实际问题。
    - 人工神经网络  
        - 霍普菲尔德神经网络（引入相连存储的机制）
        - 反向传播算法《通过误差反向传播学习表示》（能够对数据输入进行有效的表达）

    这两种技术使得人工智能的浪潮再度兴起  
    和第二次浪潮退去相似的原因，人工智能的发展在90年代又进入了冬天  

- 第三次浪潮
    2011至今  
    经历了两次浪潮之后，人们在人工智能领域变得更加理智，开始专注于发展能解决具体问题的智能技术  
    兴起的原因：
    - 计算能力的增强，尤其是图形计算能力
    - 引入了更多的高级数学工具（线代、数理统计等），创造了新的算法
        - 统计学习理论
        - 支持向量机 SVM
        - 概率图模型  
    - 数据量的增加，提供了更多的学习素材  

人工智能的提出到现在差不多50年的时间，经历了三次浪潮，在当前人工智能发展如火如荼的背景下，我们仍无法保证这一次它一定能给我们带来革命性的影响，但是人们在人工智能的探索道路上变得越来越理智，将精力更多地放到发展能够解决具体领域实际问题的智能系统上，尽管这有违最初发展人工智能的初衷：实现通用型的人工智能。

## 人工智能的定义
- 人工智能是通过机器来模拟人类认知能力的技术  
- 人工智能最重要的能力就是根据输入给出预测  
    > 在专家系统中，人工智能能够进行预测是靠人类事先给出判断机制；而后来提出的机器学习的概念中，人工智能能够在已知数据中学习得到这些判断机制，从而获得预测和判断能力。


## 机器学习
从已知数据中去学习数据中蕴含的规律或是判断规则，这里学习的目的是为了泛化，并将这些规律用在新的数据上

> 下面提到的几种学习方式并没有明显、强制的区分

- 监督学习   
    向系统中输入学习样本和这些样本对应的监督信息。在学习的过程中，系统会先根据学习样本做出自己的预测，再将自己的预测和给出的监督信息做比较获得反馈，并根据得到的反馈修改用于预测样本的数学模型。
- 无监督学习
    在不提供学习样本监督信息的条件下学习。
- 半监督学习

- 强化学习  
    这种学习方式比较特殊，利用学习得到的模型来指导行动，最终目的是找到一个最佳策略    
    强化学习具有实现真正人工智能的潜力  
    这里的模型包括：
    - 一组可以动态变化的状态
    - 一组可选的动作
    - 一个可以和决策主体交互的环境
    - 回报机制

强化学习与监督学习最主要的差别在于其收到的反馈是评估性的而不是指导性，  
监督学习会告诉学习者应当采取什么样的学习策略才能获得更高的收益，  
强化学习只会告诉学习者当前的操作是对还是错，至于哪种操作的效果最好要靠学习者自己摸索

# 第二章

特征提取和特征分类是分类任务的两个核心任务

## 提取特征
- 分类  
    根据数据的特点，判断它的类型
- 分类器  
    能够完成分类任务的人工智能系统  
- 特征  
    事物的某种属性  
    特征在很多情况下会决定分类器分类结果的好坏
- 特征向量  
    被构建成向量形式的特征
- 特征点  
    在空间中表示特征向量的点
- 特征空间  
    由特征点构成的空间  
    这里的空间可以是多维的
- 标注  
    给数据标上真实类别的行为
- 训练集  
    包含训练样本和训练样本标注的数据集

## 分类器
能够完成分类任务的人工智能系统  
由特征向量到预测类别的函数  

- 分类器的训练    
    让分类器学习得到合适参数（预测函数的参数）的过程被称为分类器的训练  
- 损失函数  
    在训练过程中用来度量分类器输出错误程度的数学化表示  
    > 预测的错误程度越大，那么损失函数的取值也就越大  
- 优化  
    调整分类器的参数，使损失函数最小的过程

<br>

- 线性分类器  
    - 感知器（perceptron）    
        利用被误分类的训练数据调整现有分类器的参数，使调整之后的分类器更加准确
    - 支持向量机(SVM)  
        SVM的结果是感知器结果集合中最完美的那一个结果，在二分类中，SVM的画出的结果恰好在两个类中间    
        > 这里提到的结果可能是线，也可以是面或是超平面
        - 支持向量  
            离SVM结果最近的特征点   
        
    SVM可以看做是限制了条件的感知器，它优化了感知器的结果

# 第三章

## 张量 tensor
空间概念  
阶数是空间的维度  
- 零阶张量  
    一个数值 标量  
- 一阶张量  
    向量  
- 二阶张量  
    矩阵

## 卷积运算

## 卷积核

## 归一化指数层
归一化指数层通常是分类器中的最后一层（soft max ?）  
对一个向量进行缩放，使向量中所有的值相加为1（这里的1可以理解为概率上的百分之一百）

## 非线性激活层
因为线性函数的复合仍然是线性函数  
如果只将卷基层和全连接层直接堆叠起来，它们对输入数据产生的效果和直接用一个全连接层的效果没有区别  
在每次线性运算之后，添加一个非线性激活层，就可以保留下线性运算之后的特征  

## 池化
为了减少卷积运算带来的数量庞大的参数，池化通过提取特征的特征，迅速减少参数

## 过拟合
训练之后的模型在训练集上表现优异，但是在测试集上表现地和狗一样

## 欠拟合
训练之后的模型在训练集和测试集上的表现都和狗一样

## 梯度下降


# 第四章
## 梅尔频率倒谱系数 MFCC
MFCC是一种特殊的频率刻度  
与普通频率的函数关系为：
$$mel(f) = 1125 × ln(1 + f / 700)$$
这种频谱比较符合人类的生理构造，人类对高频声音不敏感，因而MFCC刻度下等长的频率区间对应到普通频率下变成不等的区间。  
在每个频率区间对频谱求均值，它代表了每个频率范围内声音能量的大小。  
MFCC一共有26个区间，也就可以获得26维的向量，再经过**倒谱**之后，特征维数将降低到13维。

## 语音识别流程
1. 分帧  
    将一段音频切分成数量众多的细小的片段（帧）  
    > 片段之间可以重叠
    - 窗口宽度  
        片段宽度  
    - 窗口间隔  
        片段与片段之间的间隔
2. 获得状态
    将每一帧识别为一个状态
3. 组成音素
    简单地理解为声母和韵母  
    状态是比音素更小的语音单位  
    一般一个音素包含三个状态  

- 声学模型  
    由音频获得音素的过程
- 语言模型  
    由音素获得自然语言的过程


# 第五章



## 光流
光流描述了三维的运动点投影到二维图像之后的投影点的运动（用向量来表示？）  
光流是同一个点在相邻两帧的位移，那么计算的关键是把两帧之间的相同的点对应起来  
由上，我们可以认为：
- 相邻两帧的物体运动比较小
- 相邻两帧的颜色基本不变  

那么我们可以在相邻两帧图像中一个小范围内分别寻找颜色相近的两个点，并且在这两个点之间建立一个向量，表示点的运动过程。  

## 光流直方图 HOF
1. 切割时空单元
2. 在每个时空单元中切分8个扇区
3. 将时空单元中所有的像素点的光流向量表示在8个扇区中  
4. 获得时空单元的光流直方图
5. 过得时空单元的特征向量  
6. 对所有时空单元采取2/3/4/5的操作，拼接后获得光流直方图特征向量


## 光流灰度图
光流在每个像素上都会两个分量：水平分量和垂直分量  
那么如果我们单独将水平分量取出，然后将这些分量值缩放到0~255之间就可以得到一张灰度图像，得到水平方向上单光流灰度图。
> rgb 一种色根据明暗的不同一共可以分为256种

同理可得垂直方向的光流灰度图

## 双流卷积神经网络 （two-stream CNN）
我们将视频的信息分成静态的和动态的两个方面  
- 静态信息是指图像中物体的外观，可以由静态图片获得
- 动态信息即观察者和物体的运动，可以从光流灰度图得到

双流卷积神经网络包括
- 空间流卷积神经网络
- 事件流卷积神经网络

双流卷积神经网络拥有两个独立的卷积神经网络分别处理单帧图像和多帧光流，最后再对两个网络的行为得分进行融合。  
双流神经网络比较适合10s级别的短视频。

## 时序分段网络 TNS （temporal segment networks）
因为长视频的数据量大，所以催生出了TNS  
TNS不适用之前的密集采样而是改成稀疏采样  
TNS需要在时间轴上将视频剪切成固定数量的小片段，
再通过一定手段从每个小片段中提取一个特征，
得到固定长度的特征，并将它喂给神经网络，
最后将所有片段的预测进行融合得到最终结果。

# 第六章

## 聚类
通过分析数据在特征空间的聚集情况，将一组数据分成不同的类  
聚类的目标是使同一类中的数据尽可能的相似相近  
聚类算法是一种无监督的算法


## K值聚类
1. 确定要聚类的类别数K
2. 随机地从所有样本中选取K个样本，作为每一个类别的初始聚类中心
3. 将每一个样本划分给距离最近的聚类中心对应的类别，得到新的划分方式
4. 重新计算每类样本的聚类中心
5. 重复3/4步骤，直到聚类中心和划分方式不再发生变化

# 第七章

## 文本处理的概念
- 语料库    
    大量文本数据的集合
- 文档  
    语料库中独立的文本
- 主体   
    文档的中心思想或主要内容（可以有多个）
- 停止词   
    不携带任何主题信息的高频词汇（例如‘的’）  
    在构建词典的时候通常不会除去停止词  
- 低频词  
    次数出现极低的词语（例如人名）  
    在收集完所有词语之后会将低频词汇去除  
- 词频率  
    一个词语在文档中出现的频率  
    通常我们认为一个词语在文中出现的频率越高，它对文章的影响就越大（不是绝对的）  
- 文档频率  
    一个词语的文档频率是指所有出现过这个词语的文档数量与文档总数的商  
- 逆文档频率  
    一个词语的逆文档频率是这个词语文档频率的负对数  
    和词频率一样，如果一个词语的逆文档频率越高，这个词语也就更加重要
    > 逆文档频率是对词频率的一种修正，两种频率的积能够更加准确地突出一篇文档的特征。


## 词袋模型
将每个文档看作是装有若干词语的袋子  
只考虑词语在文档中出现的次数，而忽略语序结构，从而达到简化文档的目的   

## 主体模型
描述语料库及其潜在主体的一类数学模型  

# 第八章
## 生成对抗网络 GAN （generative adversarial network）
GAN是两种网络的混合：生成网络（generative network）和判别网络（discriminative network）  
生成网络用于生成新的数据，判别网络用于判断数据是否是计算机生成的  
生成网络让判别网络更加完善，而判别网络又会促进生成网络产生更加逼真的数据  